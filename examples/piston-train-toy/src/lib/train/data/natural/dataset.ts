import type { Config } from '$lib/workspace/config';
import type { CitationEntries } from 'example-common';

import { PUBLIC_DATA_URL } from '$env/static/public';
import { PreTrainedTokenizer } from '$lib/train/tokenizer';
import { AsyncIterableDataset } from '@piston-ml/piston-web';

import type { ToyTokenizer } from '../toy/types';

import { globalShardCache } from './shardCache';

export type NaturalDatasetSplit = 'train' | 'val';

export type NaturalDatasetName = 'tinychat' | 'tinyshakespeare' | 'tinystories' | 'fineweb';
export type NaturalDatasetMeta = {
	name: string;
	description: string;
	citations: CitationEntries;
};
export const NATURAL_DATASET_META: Record<NaturalDatasetName, NaturalDatasetMeta> = {
	tinystories: {
		name: 'TinyStories (v2)',
		description: 'Short stories generated by GPT-4 using a limited vocabulary.',
		citations: {
			entries: [
				{
					name: 'Eldan, 2023',
					url: 'https://arxiv.org/abs/2305.07759'
				}
			]
		}
	},
	tinyshakespeare: {
		name: 'TinyShakespeare',
		description: "40,000 lines of Shakespeare from a variety of Shakespeare's plays.",
		citations: {
			entries: [
				{
					name: 'Karpathy, 2015',
					url: 'https://github.com/karpathy/char-rnn'
				}
			]
		}
	},
	tinychat: {
		name: 'TinyChat',
		description: 'Short conversations generated by GPT-4o.',
		citations: {
			entries: [
				{
					name: 'Raghavendra, 2024',
					url: 'https://web.archive.org/https://nikhilr.io/posts/TinyChat15M/'
				}
			]
		}
	},
	fineweb: {
		name: 'FineWeb',
		description: 'Cleaned and deduplicated English web data from CommonCrawl.',
		citations: {
			entries: [
				{
					name: 'Penedo et al., 2024',
					url: 'https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1'
				}
			]
		}
	}
};

export interface NaturalDatasetConfig {
	name: NaturalDatasetName;
	split: NaturalDatasetSplit;
	contextSize: number;
	masked: boolean;
	vocabSize: 'char' | 512 | 1024 | 2048 | 4096 | 8192 | 16384 | 32768 | 65536;
}

function joinUrl(base: string, path: string): string {
	const trimmedBase = base.endsWith('/') ? base.slice(0, -1) : base;
	const trimmedPath = path.startsWith('/') ? path.slice(1) : path;
	return `${trimmedBase}/${trimmedPath}`;
}

function pad4(n: number): string {
	return n.toString().padStart(4, '0');
}

// Format is from Karpathy's llm.c: https://github.com/karpathy/llm.c
function parseShard(buffer: ArrayBuffer): Uint16Array {
	if (buffer.byteLength < 256 * 4) {
		throw new Error('Shard too small to contain header');
	}
	const header = new Int32Array(buffer, 0, 256);
	const magic = header[0];
	const version = header[1];
	const ntok = header[2];
	if (magic !== 20251003) {
		throw new Error(`magic number mismatch in the data .bin file (got ${magic})`);
	}
	if (version !== 1) {
		throw new Error(`unsupported version ${version}`);
	}
	const tokens = new Uint16Array(buffer, 256 * 4);
	if (tokens.length !== ntok) {
		throw new Error(`number of tokens read (${tokens.length}) does not match header (${ntok})`);
	}
	return tokens;
}

async function fetchShard(url: string): Promise<Uint16Array | null> {
	// Try cache first
	const cached = await globalShardCache.get(url).catch(() => undefined);
	if (cached) {
		return parseShard(cached);
	}
	const res = await fetch(url);
	if (res.status === 404) return null;
	if (!res.ok) {
		throw new Error(`Failed to fetch shard ${url}: ${res.status} ${res.statusText}`);
	}
	const buf = await res.arrayBuffer();
	// Store in cache (best-effort)
	void globalShardCache.set(url, buf);
	return parseShard(buf);
}

// const charTokenizer: ToyTokenizer = {
// 	vocab: Object.fromEntries(Array.from({ length: 256 }, (_, i) => [String.fromCharCode(i), i])),
// 	ids: Object.fromEntries(Array.from({ length: 256 }, (_, i) => [i, String.fromCharCode(i)])),
// 	lastToken: 0,
// 	decode: (tokens: number[]) => tokens.map((token) => String.fromCharCode(token)).join('')
// };

function buildCharTokenizer(masked: boolean): ToyTokenizer {
	const vocabList = Array.from({ length: 256 }, (_, i) => String.fromCharCode(i));
	vocabList.push('<eos>');
	vocabList[32] = '‚ê£';
	if (masked) {
		vocabList.push('<mask>');
	}
	const vocab = Object.fromEntries(vocabList.map((char, i) => [char, i]));
	const ids = Object.fromEntries(vocabList.map((char, i) => [i, char]));
	return {
		vocab,
		ids,
		lastToken: vocabList.length - 1,
		decode: (tokens: number[]) => tokens.map((token) => ids[token] ?? '<unk>').join('')
	};
}

type NaturalLanguageShard = {
	data: Uint16Array;
	cursor: number;
};

export class NaturalLanguageDataset extends AsyncIterableDataset<number[]> {
	readonly name: NaturalDatasetName;
	readonly split: NaturalDatasetSplit;
	readonly contextSize: number;
	readonly charLevel: boolean;
	vocabSize: number | Promise<number>;
	maskId: number | Promise<number> | null;
	eosId: number | Promise<number>;
	bosId: number | Promise<number>;
	tokenizer: PreTrainedTokenizer | Promise<PreTrainedTokenizer> | ToyTokenizer | null;

	private shard: NaturalLanguageShard | null = null;
	private shardIndex: number = 0;
	private nextShardPromise: Promise<Uint16Array | null> | null = null;

	constructor(config: NaturalDatasetConfig) {
		super();
		this.name = config.name;
		this.split = config.split;
		this.contextSize = config.contextSize;
		this.charLevel = Boolean(config.vocabSize === 'char');
		const baseVocabSize = config.vocabSize === 'char' ? 257 : config.vocabSize;

		this.tokenizer = null;

		if (config.vocabSize === 'char') {
			this.tokenizer = buildCharTokenizer(config.masked);
			if (config.masked) {
				this.vocabSize = baseVocabSize + 1;
				this.maskId = this.vocabSize - 1;
				this.eosId = this.vocabSize - 2;
				this.bosId = this.vocabSize - 2;
			} else {
				this.vocabSize = baseVocabSize;
				this.maskId = null;
				this.eosId = baseVocabSize - 1;
				this.bosId = baseVocabSize - 1;
			}
		} else {
			this.vocabSize = config.vocabSize;
			this.tokenizer = PreTrainedTokenizer.fromPretrained(`${this.name}/${this.vocabSize}`);
			this.maskId = this.tokenizer.then((tokenizer) => tokenizer.maskTokenId!);
			this.eosId = this.tokenizer.then((tokenizer) => tokenizer.eosTokenId!);
			this.bosId = this.tokenizer.then((tokenizer) => tokenizer.bosTokenId!);

			this.tokenizer.then((tokenizer) => {
				this.tokenizer = tokenizer;
				this.maskId = tokenizer.maskTokenId!;
				this.eosId = tokenizer.eosTokenId!;
				this.bosId = tokenizer.bosTokenId!;
			});
		}
	}

	private buildShardUrl(shardIndex: number): string {
		const file = `${this.split}-${pad4(shardIndex)}.bin`;
		return joinUrl(
			PUBLIC_DATA_URL,
			`/tokenized/${this.name}/${this.charLevel ? 'char' : this.vocabSize}/${file}?v=1`
		);
	}

	private async ensureCurrentLoaded(): Promise<void> {
		if (this.shard) return;
		const first = await fetchShard(this.buildShardUrl(0));
		if (!first) {
			// No data available; mark as exhausted
			this.shard = null;
			return;
		}
		this.shard = { data: first, cursor: 0 };
		if (this.split === 'train') {
			this.nextShardPromise = fetchShard(this.buildShardUrl(1));
		}
	}

	private async advanceShard(): Promise<void> {
		if (this.split === 'val') {
			// Validation: single shard only; end iteration
			this.shard = null;
			return;
		}
		// Train: move to next shard, keeping one-shard lookahead
		const next = this.nextShardPromise ? await this.nextShardPromise : null;
		if (!next) {
			this.shard = null;
			return;
		}
		this.shardIndex += 1;
		this.shard = { data: next, cursor: 0 };
		// Prefetch following shard
		this.nextShardPromise = fetchShard(this.buildShardUrl(this.shardIndex + 1));
	}

	public async *[Symbol.asyncIterator](): AsyncIterator<number[]> {
		await this.ensureCurrentLoaded();
		while (true) {
			if (!this.shard) {
				return;
			}
			const tokens = this.shard.data;
			const start = this.shard.cursor;
			const end = start + this.contextSize;
			if (end <= tokens.length) {
				const slice = tokens.slice(start, end);
				this.shard.cursor = end;
				yield Array.from(slice);
				continue;
			}
			// Advance if we need more tokens than are available in the current shard
			await this.advanceShard();
		}
	}

	// Export/import state for resumption
	public exportState(): { shardIndex: number; cursor: number } | null {
		if (!this.shard) return { shardIndex: this.shardIndex, cursor: 0 };
		return { shardIndex: this.shardIndex, cursor: this.shard.cursor };
	}

	public async importState(state: { shardIndex: number; cursor: number } | null): Promise<void> {
		if (!state) return;
		this.shardIndex = Math.max(0, state.shardIndex | 0);
		const current = await fetchShard(this.buildShardUrl(this.shardIndex));
		if (!current) {
			this.shard = null;
			this.nextShardPromise = null;
			return;
		}
		this.shard = { data: current, cursor: Math.min(state.cursor | 0, current.length) };
		if (this.split === 'train') {
			this.nextShardPromise = fetchShard(this.buildShardUrl(this.shardIndex + 1));
		} else {
			this.nextShardPromise = null;
		}
	}
}

export function buildNaturalLanguageDataset(
	split: NaturalDatasetSplit,
	config: Config
): NaturalLanguageDataset {
	return new NaturalLanguageDataset({
		name: config.data.dataset as NaturalDatasetName,
		split,
		contextSize: config.data.natural.contextSize,
		masked: config.model.topology === 'encoder',
		vocabSize: config.data.natural.vocabSize
	});
}

export default NaturalLanguageDataset;
